{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPath = \"./cacm/cacm.all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser\n",
    "Returns dictionary named “documents”. indexes are the document ids. documents[index] = {‘title’,\n",
    "‘abstract’, ‘date’, ‘authors’} We use dictionary, because we can search and get document details\n",
    "with O(1) complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(myPath = \"./cacm/cacm.all\"):\n",
    "    documents = {}\n",
    "    my_file = open(myPath,'r',encoding='windows-1252')\n",
    "    while True:\n",
    "        line = my_file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        elif line.startswith('.I'):\n",
    "            mode = 'i'\n",
    "            index = line.split(' ')[-1:][0]\n",
    "            index = index.replace('\\n', '')\n",
    "        elif line.startswith('.T'):\n",
    "            mode = 't'\n",
    "        elif line.startswith('.W'):\n",
    "            mode = 'w'\n",
    "        elif line.startswith('.B'):\n",
    "            mode = 'b'\n",
    "        elif line.startswith('.A'):\n",
    "            mode = 'a'\n",
    "        elif line.startswith('.'):\n",
    "            mode = 'z'\n",
    "        else:\n",
    "            if mode=='t':\n",
    "                documents[index] = {'title': line.replace('\\n', ''), 'abstract': '', 'date': '', 'authors': ''}\n",
    "            elif mode=='w':\n",
    "                documents[index]['abstract'] += line.replace('\\n', '')\n",
    "            elif mode=='b':\n",
    "                documents[index]['date'] = line.replace('\\n', '')\n",
    "            elif mode=='a':\n",
    "                documents[index]['authors'] += line.replace('\\n', '')\n",
    "            elif mode=='z':\n",
    "                continue\n",
    "    my_file.close()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Tokenizing\n",
    "## Removing stopwords\n",
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(documents):\n",
    "    # Tokenizing using list comprehension and regular expressions\n",
    "    tokenizing = [re.findall('\\w+',documents[documentId][\"terms\"]) for documentId in documents]\n",
    "    token_length = 0\n",
    "    for doc in tokenizing:\n",
    "        token_length+=len(doc)\n",
    "    print(f'Number of tokens before preprocessing: {token_length}')\n",
    "    # Open stopwords file\n",
    "    stop_words = open(\"./stopwords.txt\",'r',encoding='windows-1252')\n",
    "    stop_words = stop_words.read()\n",
    "    stop_words = stop_words.split()\n",
    "    stopWords_removed = []\n",
    "    stems = []\n",
    "    finished_dic = {}\n",
    "    # Remove stopwords using a list comprehension\n",
    "    for doc in tokenizing:\n",
    "        doc = [d for d in doc if d not in stop_words]\n",
    "        stopWords_removed.append(doc)\n",
    "\n",
    "    stopWord_length = 0\n",
    "    for doc in stopWords_removed:\n",
    "        stopWord_length += len(doc)\n",
    "    print(f'Number of tokens after removing stop words: {stopWord_length}')\n",
    "    # Using Porter Stemmer algorithm\n",
    "    porter = PorterStemmer()\n",
    "    for doc in stopWords_removed:\n",
    "        doc = [porter.stem(s) for s in doc]\n",
    "        stems.append(doc)\n",
    "\n",
    "    stem_length = 0\n",
    "    for doc in stems:\n",
    "        stem_length+=len(doc)\n",
    "    print(f'Number of tokens after stemming: {stem_length}')\n",
    "\n",
    "    for documentId in documents:\n",
    "        finished_dic[documentId] = {'terms': ''}\n",
    "        finished_dic[documentId]['terms'] = ' '.join(stems[int(documentId)-1])\n",
    "\n",
    "    return finished_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create inverted index\n",
    "returns dictionary inverted_index. #### Inverted_index: word : {‘doc_id’ : documentId_list,\n",
    "‘token_id’ : token_id, ‘tf’ : term_frequency}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(preprocessed):\n",
    "    inverted_index={}\n",
    "    token_id = 1\n",
    "    for documentId, text in preprocessed.items():\n",
    "        for word in text['terms'].lower().split():\n",
    "            # If the term is in dictionary\n",
    "            if inverted_index.get(word,False):\n",
    "                # Add term frequency\n",
    "                inverted_index[word]['tf']+=1\n",
    "                if documentId not in inverted_index[word]['doc_id']:\n",
    "                    inverted_index[word]['doc_id'].append(documentId)\n",
    "            else:\n",
    "                # If the term is not in inverted_index dictionary\n",
    "                inverted_index[word]={'doc_id':[documentId], 'token_id':token_id, 'tf':1}\n",
    "                token_id+=1\n",
    "\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = parser(myPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge title and abstract parts into new “terms” value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for docId in documents:\n",
    "    documents[docId]['terms'] = documents[docId]['title'] + documents[docId]['abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and printing information about tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens before preprocessing: 161485\n",
      "Number of tokens after removing stop words: 113749\n",
      "Number of tokens after stemming: 113749\n"
     ]
    }
   ],
   "source": [
    "preprocessed = preprocessing(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of preprocessed documents: 3204\n"
     ]
    }
   ],
   "source": [
    "doc_count = len(preprocessed)\n",
    "print(f'Number of preprocessed documents: {doc_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create inverted-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in inverted-index: 13351\n"
     ]
    }
   ],
   "source": [
    "inverted_index = create_index(preprocessed)\n",
    "print(f'Number of words in inverted-index: {len(inverted_index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "durations = []\n",
    "while True:\n",
    "    term = input('please Enter the term: ')\n",
    "    # Keep start time in mind\n",
    "    start_time = time.time()\n",
    "    # If user enters \"ZZEND\", we break the loop and end the program\n",
    "    if term == 'ZZEND':\n",
    "        break\n",
    "    print(f'Document frequency: {len(inverted_index[term][\"doc_id\"])}')\n",
    "    for t in inverted_index[term]['doc_id']:\n",
    "        # print document id\n",
    "        print(f'Document id: {t}')\n",
    "        # print document title\n",
    "        print(f'Title: {documents[t][\"title\"]}')\n",
    "        # print term frequency in the document\n",
    "        print(f'Term frequency: {len(re.findall(term,preprocessed[t][\"terms\"]))}')\n",
    "        # Find out the occurrences of term in the document\n",
    "        occurrences = [i.start() for i in re.finditer(term, preprocessed[t]['terms'])]\n",
    "        print('occurrences: ', occurrences)\n",
    "        # Creat document summary with 8 words in it's context\n",
    "        doc = preprocessed[t]['terms']\n",
    "        doc = doc.replace(term, term.upper())\n",
    "        doc = doc.split(' ')\n",
    "        start = doc.index(term.upper())\n",
    "        summary = ''\n",
    "        try:\n",
    "            for i in range(start, start+8):\n",
    "                summary += doc[i] + ' '\n",
    "        except:\n",
    "            pass\n",
    "        print(f'Document summary: {summary}')\n",
    "        # Print query execution time;\n",
    "        # append duration to durations list\n",
    "        durations.append(time.time()-start_time)\n",
    "        print('Query execution time: ',time.time()-start_time)\n",
    "sum = 0\n",
    "# Find average execution time based on durations list\n",
    "try:\n",
    "    for duration in durations:\n",
    "        sum += duration\n",
    "    average_time = sum / len(durations)\n",
    "    print('Average query execution time: ', average_time)\n",
    "except: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run the program?\n",
    "#### Unzip \"IR_InvertedIndex.zip\", open project folder, then run the program by following command:\n",
    "\n",
    "##### python IR_InvertedIndex.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
