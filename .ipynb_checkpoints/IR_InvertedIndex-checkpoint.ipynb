{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "## PorterStemmer\n",
    "Stemming algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser\n",
    "returns dictionary named \"documents\". indexes are the document ids. \n",
    "documents[index] = {'title', 'abstract', 'date', 'authors'}\n",
    "We use dictionary, because we can search and get document details with O(1) complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(myPath = \"./cacm/cacm.all\"):\n",
    "    documents = {}\n",
    "    my_file = open(myPath,'r',encoding='windows-1252')\n",
    "    while True: \n",
    "        line = my_file.readline() \n",
    "        if not line: \n",
    "            break\n",
    "        elif line.startswith('.I'):\n",
    "            mode = 'i'\n",
    "            index = line.split(' ')[-1:][0]\n",
    "            index = index.replace('\\n', '')\n",
    "        elif line.startswith('.T'):\n",
    "            mode = 't'\n",
    "        elif line.startswith('.W'):\n",
    "            mode = 'w'\n",
    "        elif line.startswith('.B'):\n",
    "            mode = 'b'\n",
    "        elif line.startswith('.A'):\n",
    "            mode = 'a'\n",
    "        elif line.startswith('.'):\n",
    "            mode = 'z'\n",
    "        else:\n",
    "            if mode=='t':\n",
    "                documents[index] = {'title': line.replace('\\n', ''), 'abstract': '', 'date': '', 'authors': ''}\n",
    "            elif mode=='w':\n",
    "                documents[index]['abstract'] += line.replace('\\n', '')\n",
    "            elif mode=='b':\n",
    "                documents[index]['date'] = line.replace('\\n', '')\n",
    "            elif mode=='a':\n",
    "                documents[index]['authors'] += line.replace('\\n', '')\n",
    "            elif mode=='z':\n",
    "                continue\n",
    "    my_file.close() \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge title and abstract parts into new \"terms\" value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in documents:\n",
    "    documents[key]['terms'] = documents[key]['title'] + documents[key]['abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Tokenizing\n",
    "## Removing stopwords\n",
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(documents):\n",
    "    # Tokenizing using list comprehension and regular expressions \n",
    "    tokenizing = [re.findall('\\w+',documents[documentId][\"terms\"]) for documentId in documents]\n",
    "    token_length = 0\n",
    "    for doc in tokenizing:\n",
    "        token_length+=len(doc)\n",
    "    print(f'Number of tokens before preprocessing: {token_length}')\n",
    "    # Open stopwords file\n",
    "    stop_words = open(\"./stopwords.txt\",'r',encoding='windows-1252')\n",
    "    stop_words = stop_words.read()\n",
    "    stop_words = stop_words.split()\n",
    "    stopWords_removed = []\n",
    "    stems = []\n",
    "    finished_dic = {} \n",
    "    # Remove stopwords using a list comprehension\n",
    "    for doc in tokenizing:\n",
    "        doc = [d for d in doc if d not in stop_words]\n",
    "        stopWords_removed.append(doc)\n",
    "    \n",
    "    stopWord_length = 0\n",
    "    for doc in stopWords_removed:\n",
    "        stopWord_length += len(doc)\n",
    "    print(f'Number of tokens after removing stop words: {stopWord_length}')\n",
    "    # Using Porter Stemmer algorithm\n",
    "    porter = PorterStemmer()\n",
    "    for doc in stopWords_removed:\n",
    "        doc = [porter.stem(s) for s in doc]\n",
    "        stems.append(doc)\n",
    "    \n",
    "    stem_length = 0\n",
    "    for doc in stems:\n",
    "        stem_length+=len(doc)\n",
    "    print(f'Number of tokens after stemming: {stem_length}')\n",
    "\n",
    "    for documentId in documents:\n",
    "        finished_dic[documentId] = {'terms': ''}\n",
    "        finished_dic[documentId]['terms'] = ' '.join(stems[int(documentId)-1])\n",
    "\n",
    "    return finished_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create inverted index\n",
    "returns dictionary inverted_index.\n",
    "#### Inverted_index:\n",
    "word : {'doc_id' : documentId_list, 'token_id' : token_id, 'tf' : term_frequency}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(preprocessed):\n",
    "    inverted_index={}\n",
    "    token_id = 1\n",
    "    for documentId, text in preprocessed.items():\n",
    "        for word in text['terms'].lower().split():\n",
    "            # If the term is in dictionary\n",
    "            if inverted_index.get(word,False):\n",
    "                # Add term frequency\n",
    "                inverted_index[word]['tf']+=1\n",
    "                if documentId not in inverted_index[word]['doc_id']:\n",
    "                    inverted_index[word]['doc_id'].append(documentId)\n",
    "            else:\n",
    "                # If the term is not in inverted_index dictionary \n",
    "                inverted_index[word]={'doc_id':[documentId], 'token_id':token_id, 'tf':1}\n",
    "                token_id+=1\n",
    "                \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = parser(myPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for docId in documents:\n",
    "    documents[docId]['terms'] = documents[docId]['title'] + documents[docId]['abstract'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information about tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens before preprocessing: 161485\n",
      "Number of tokens after removing stop words: 113749\n",
      "Number of tokens after stemming: 113749\n"
     ]
    }
   ],
   "source": [
    "preprocessed = preprocessing(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of preprocessed documents: 3204\n"
     ]
    }
   ],
   "source": [
    "doc_count = len(preprocessed)\n",
    "print(f'Number of preprocessed documents: {doc_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in inverted-index: 13351\n"
     ]
    }
   ],
   "source": [
    "inverted_index = create_index(preprocessed)\n",
    "print(f'Number of words in inverted-index: {len(inverted_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please Enter the term: preliminari\n",
      "Document frequency: 17\n",
      "Document id: 1\n",
      "Title: Preliminary Report-International Algebraic Language\n",
      "Term frequency: 1\n",
      "occurrences:  [0]\n",
      "Document summary: PRELIMINARI report intern algebra languag \n",
      "Query execution time:  0.00016927719116210938\n",
      "Document id: 254\n",
      "Title: SMALGOL-61\n",
      "Term frequency: 1\n",
      "occurrences:  [178]\n",
      "Document summary: PRELIMINARI report result At acm nation confer four \n",
      "Query execution time:  0.00028824806213378906\n",
      "Document id: 825\n",
      "Title: for the Analysis of Spark-Chamber Data*\n",
      "Term frequency: 1\n",
      "occurrences:  [228]\n",
      "Document summary: PRELIMINARI interpret these photograph In continu oper processingr \n",
      "Query execution time:  0.0015037059783935547\n",
      "Document id: 894\n",
      "Title: An Iterative Factorization Technique for Polynomials\n",
      "Term frequency: 1\n",
      "occurrences:  [264]\n",
      "Document summary: PRELIMINARI result indic well adaptedto use digit comput \n",
      "Query execution time:  0.001638650894165039\n",
      "Document id: 1205\n",
      "Title: An Undergraduate Program in Computer Science-Preliminary Recommendations\n",
      "Term frequency: 1\n",
      "occurrences:  [36]\n",
      "Document summary: PRELIMINARI recommend \n",
      "Query execution time:  0.0017695426940917969\n",
      "Document id: 1235\n",
      "Title: A Stochastic Approach to the Grammatical Coding of English\n",
      "Term frequency: 1\n",
      "occurrences:  [547]\n",
      "Document summary: PRELIMINARI trial accuraci coder 91 93 withobvi way \n",
      "Query execution time:  0.001911163330078125\n",
      "Document id: 1771\n",
      "Title: ACM Curriculum Committee on Computer science\n",
      "Term frequency: 1\n",
      "occurrences:  [384]\n",
      "Document summary: PRELIMINARI recommend given undergradu program graduat program computersci \n",
      "Query execution time:  0.0020494461059570312\n",
      "Document id: 1946\n",
      "Title: vs. Collins' Reduced P.R.S. Algorithm\n",
      "Term frequency: 1\n",
      "occurrences:  [307]\n",
      "Document summary: PRELIMINARI consider narrowth choic best algorithm bezout s \n",
      "Query execution time:  0.0023527145385742188\n",
      "Document id: 2050\n",
      "Title: Automatic Parsing for Content Analysis\n",
      "Term frequency: 1\n",
      "occurrences:  [201]\n",
      "Document summary: PRELIMINARI studi show harvard syntact analyz canproduc correct \n",
      "Query execution time:  0.002491474151611328\n",
      "Document id: 2065\n",
      "Title: A Programming System for the On-line Analysis of Biomedical Images\n",
      "Term frequency: 1\n",
      "occurrences:  [48]\n",
      "Document summary: PRELIMINARI descript softwar comput displaysystem given special emphasi \n",
      "Query execution time:  0.006499052047729492\n",
      "Document id: 2163\n",
      "Title: Education Related to the Use of Computers in Organizations\n",
      "Term frequency: 1\n",
      "occurrences:  [248]\n",
      "Document summary: PRELIMINARI conclus present thene educ administr inform system \n",
      "Query execution time:  0.006680488586425781\n",
      "Document id: 2181\n",
      "Title: The State of Computer Oriented Curricula in Business Schools 1970\n",
      "Term frequency: 1\n",
      "occurrences:  [741]\n",
      "Document summary: PRELIMINARI committe s recommendationsfor improv comput educ manag \n",
      "Query execution time:  0.006793498992919922\n",
      "Document id: 2389\n",
      "Title: Preliminary Report on a System for General Space Planning\n",
      "Term frequency: 1\n",
      "occurrences:  [0]\n",
      "Document summary: PRELIMINARI report system gener space planninga comput languag \n",
      "Query execution time:  0.006892204284667969\n",
      "Document id: 2556\n",
      "Title: Adaptive Correction of Program Statements\n",
      "Term frequency: 1\n",
      "occurrences:  [332]\n",
      "Document summary: PRELIMINARI oper result present A final section survey \n",
      "Query execution time:  0.006989955902099609\n",
      "Document id: 2718\n",
      "Title: A Preliminary System for the Design of DBTG Data Structures\n",
      "Term frequency: 1\n",
      "occurrences:  [2]\n",
      "Document summary: PRELIMINARI system design dbtg data structuresth function approach \n",
      "Query execution time:  0.007089853286743164\n",
      "Document id: 2929\n",
      "Title: An Analysis of Inline Substitution for a Structured Programming Language\n",
      "Term frequency: 1\n",
      "occurrences:  [342]\n",
      "Document summary: PRELIMINARI result theclu structur program languag indic program \n",
      "Query execution time:  0.00721287727355957\n",
      "Document id: 2970\n",
      "Title: Achieving Specific Accuracy in Simulation Output Analysis\n",
      "Term frequency: 1\n",
      "occurrences:  [476]\n",
      "Document summary: PRELIMINARI simul run the paper also describ varianc \n",
      "Query execution time:  0.0073893070220947266\n",
      "please Enter the term: ZZEND\n",
      "Average query execution time:  0.0041004769942339725\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "durations = []\n",
    "while True:\n",
    "    term = input('please Enter the term: ')\n",
    "    start_time = time.time()\n",
    "    if term == 'ZZEND': \n",
    "        break\n",
    "    print(f'Document frequency: {len(inverted_index[term][\"doc_id\"])}')\n",
    "    for t in inverted_index[term]['doc_id']:\n",
    "        print(f'Document id: {t}')\n",
    "        print(f'Title: {documents[t][\"title\"]}')\n",
    "        print(f'Term frequency: {len(re.findall(term,preprocessed[t][\"terms\"]))}')\n",
    "        occurrences = [i.start() for i in re.finditer(term, preprocessed[t]['terms'])]\n",
    "        print('occurrences: ', occurrences)\n",
    "        doc = preprocessed[t]['terms']\n",
    "        doc = doc.replace(term, term.upper())\n",
    "        doc = doc.split(' ')\n",
    "        start = doc.index(term.upper()) \n",
    "        summary = ''\n",
    "        try:\n",
    "            for i in range(start, start+8):\n",
    "                summary += doc[i] + ' '\n",
    "        except:\n",
    "            pass\n",
    "        print(f'Document summary: {summary}')\n",
    "        durations.append(time.time()-start_time)\n",
    "        print('Query execution time: ',time.time()-start_time)\n",
    "sum = 0\n",
    "for duration in durations:\n",
    "    sum += duration\n",
    "average_time = sum / len(durations)\n",
    "print('Average query execution time: ', average_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
